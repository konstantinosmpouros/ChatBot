{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53e4898d-792b-44ab-9033-6546991ac751",
   "metadata": {},
   "source": [
    "# ChatBot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ad043-ce86-43fd-bd78-94139daff13f",
   "metadata": {},
   "source": [
    "> Konstantinos Mpouros <br>\n",
    "> Github: https://github.com/konstantinosmpouros <br>\n",
    "> Year: 2025 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a425f285-2b4e-48e8-95d9-44a95acaf316",
   "metadata": {},
   "source": [
    "This project focuses on building an intelligent chatbot capable of providing accurate and context-aware responses. Designed to simulate human-like conversations, the chatbot is versatile enough for applications such as customer support, personal assistants, or educational tools.  \n",
    "\n",
    "The project leverages state-of-the-art large language models (LLMs), offering the flexibility to choose between an open-source model from Hugging Face or GPT-4o. This dual approach ensures adaptability to different requirements, balancing performance, cost, and customization potential.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e17a95-7001-4c22-876d-80836e1ddd7b",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65c1bfe7-10f8-42af-bb21-3358484a4afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import gc\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e818730-b47e-4abb-b620-c92353196256",
   "metadata": {},
   "source": [
    "## 2. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f26ed0-c918-42d9-b511-b1ad8f34bfdc",
   "metadata": {},
   "source": [
    "* Load enviroment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "843c7930-e16a-4060-bd30-e9dc6eaaa641",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['HUGGINGFACE_TOKEN'] = os.getenv('HUGGINGFACE_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b0ed07-ca5a-4281-8eba-7f2cf0395b8e",
   "metadata": {},
   "source": [
    "* Login to hugging face hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7337012c-265e-47b9-9c67-a240aae23273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/kostasbouros/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(token=os.getenv('HUGGINGFACE_TOKEN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023708ee-5a05-4606-ba99-e218c186c2fd",
   "metadata": {},
   "source": [
    "* Define initial prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc2eba1a-4890-4458-b840-5d4b3678ed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_prompts():\n",
    "    system_message = \"\"\"\n",
    "        You are a helpful ai assistant that answer only in english of an airline company named aegean willing to help with anything that the user asks about the company and nothing different.\n",
    "        You must always remain kind, and try first to understand what the user want and then respond.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25343f1-3ec4-499c-a8c3-12a49d48648b",
   "metadata": {},
   "source": [
    "* Define a method to add in the history the user prompt and the response of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14565883-70f8-4012-8cfe-e2e617b7538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_history(history, role, prompt):\n",
    "    history.append({'role': role, 'content': prompt})\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ac6bb-c0e8-4dbf-8889-b11c0010b097",
   "metadata": {},
   "source": [
    "* Define a function to remind in the model not to respond for other stuff exept those in the system message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bf42f77-558b-48a2-82d9-1fa0d1bdaf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_remider():\n",
    "    prompt = \"\"\"\n",
    "    Remember, you are an ai assistant of the Aegean company and answer only question that has to do with the company's info.\n",
    "    if the user want to know something different answer kindly that you cant help with topic not relevand to aegean.\n",
    "    Answer only in english, brief and clear.\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f234ea-a458-4499-90d8-f51b1f6b835a",
   "metadata": {},
   "source": [
    "* Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31144b03-ef80-4c09-9799-f25de8c45d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Set quantization method\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "        \n",
    "        # Load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                     device_map=\"cuda\",\n",
    "                                                     quantization_config=quant_config)\n",
    "\n",
    "        return tokenizer, model\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex.args)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return None, None       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193ebe6d-6b7a-4533-bb59-48171ed05cdc",
   "metadata": {},
   "source": [
    "* Define a method to chat with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea7e89d3-4e26-4d6a-a9f3-66a1e95de074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_response(tokenizer, model, history, prompt):\n",
    "    # Set up the message to be tokenized\n",
    "    add_to_history(history, role='user', prompt=prompt)\n",
    "    add_to_history(history, role='system', prompt=add_remider())\n",
    "    tokenized_message = tokenizer.apply_chat_template(history, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    # Generate response\n",
    "    response = model.generate(tokenized_message, temperature=0.85, max_new_tokens=10000)\n",
    "    generated_tokens = response[0][len(tokenized_message[0]):]\n",
    "\n",
    "    # Decode response\n",
    "    output = tokenizer.decode(generated_tokens , skip_special_tokens=True)\n",
    "    if \"assistant\" in output:\n",
    "        output = output.split(\"assistant\", 1)[-1].strip()\n",
    "    history = add_to_history(history, role='assistant', prompt=output)\n",
    "\n",
    "    return output, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46bddeed-24e5-4c1d-84e2-ffb904c8fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(tokenizer, model, history, prompt):\n",
    "    llm_response(tokenizer, model, history, prompt)\n",
    "    print(history[-1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97115664-c448-40ac-b4ea-529b782490c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Chating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b467b-4864-4553-a5ec-c3f7413d77bd",
   "metadata": {},
   "source": [
    "* Lets chat with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d20cb91e-5c69-4744-8873-035261b9379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = init_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bb6334e-de1d-4fd8-88cb-2003ecf72fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bcfee3c7d33409ab8fd731dd5e65aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer, model = load_model('meta-llama/Meta-Llama-3.1-8B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14db1336-94ea-4ab8-b503-d5238cb82b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "2024-12-25 14:33:29.845039: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-25 14:33:29.913850: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-25 14:33:29.936274: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-25 14:33:30.071907: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-25 14:33:30.941420: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Konstantinos! I'm an assistant for Aegean Airlines. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "chat(tokenizer, model, history, 'Hi!!, l am konstantinos, you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a77b6397-1f99-4d6f-a4da-4c9ea7c89422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name \"Aegean\" originates from the Aegean Sea, which is a significant body of water in Greece, surrounding many islands where the airline operates. This geographical connection reflects the airline's Greek roots and its primary service area.\n"
     ]
    }
   ],
   "source": [
    "chat(tokenizer, model, history, 'l would like to know where the name aegean come from!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ab5accf-5406-473f-8de5-70e17698358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a reliable and reputable airline, Aegean has received numerous awards and accolades, including \"Best Regional Airline in Europe\" from the Air Transport Awards. We strive to provide excellent service and safe travels to our passengers. Would you like to know more about our services or routes?\n"
     ]
    }
   ],
   "source": [
    "chat(tokenizer, model, history, 'l would like to know if it a good company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ade6737-584f-4052-9378-b1d43a659776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm glad you found that information helpful, Konstantinos. If you're ready to move forward, what else would you like to know about Aegean Airlines? Our fleet, destinations, or something else?\n"
     ]
    }
   ],
   "source": [
    "chat(tokenizer, model, history, 'Ok thats some nice info!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f213dabf-2c8d-4036-8b96-462956c1fe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to help with Aegean-related topics, Konstantinos, but unfortunately, I'm not able to assist with football-related questions as it's not related to Aegean Airlines. If you'd like to know something about our flight routes, services, or more, I'd be happy to help.\n"
     ]
    }
   ],
   "source": [
    "chat(tokenizer, model, history, 'l would like to ask you about football actually, can you assist?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dba5d3ad-c9d1-45f7-9043-f836801dbfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panathinaikos is a well-known Greek sports club, but unfortunately, it's not directly related to Aegean Airlines. I'd be happy to help with a different question about Aegean Airlines, though. Perhaps you'd like to know about our partnership with Panathinaikos or any other Aegean-related topic?\n"
     ]
    }
   ],
   "source": [
    "chat(tokenizer, model, history, 'tell me about panathinaikos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf197677-0544-42e0-89a0-e505b6e58e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to clarify, but unfortunately, I still can't assist with information about Panathinaikos' football team. My expertise lies within Aegean Airlines, and I'd be happy to provide information on our routes, services, or more. If you'd like to know something about Aegean's partnership with a sports club, I could try to provide that information.\n"
     ]
    }
   ],
   "source": [
    "chat(tokenizer, model, history, 'l dont want that, l want to know about the football only pls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e458246-b0ce-4384-b0a6-eae8584869aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm afraid I'm not able to provide information about Panathinaikos, as it's not related to Aegean Airlines. My purpose is to assist with questions about Aegean Airlines, and I'd be happy to help with that. If you'd like to know something about our services, routes, or more, I'd be happy to help.\n"
     ]
    }
   ],
   "source": [
    "chat(tokenizer, model, history, 'l want to know more about panathinaikos not aegean pls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5705392-4b96-4a4a-b233-3da86085aba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Konstantinos. I remember your name. How can I help you with an Aegean Airlines-related question now?\n"
     ]
    }
   ],
   "source": [
    "chat(tokenizer, model, history, 'ok but do you remember my name?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17a8835-7c55-4dc3-b74b-e7bcc53c9ce7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. UI Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc43134d-b8b2-4dc0-8f51-02d099bed12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kostasbouros/.local/lib/python3.10/site-packages/gradio/components/chatbot.py:242: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63bb42b89d0d4ffdab7bf6cbb76fb125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7875\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define your functions\n",
    "def gradio_chat(user_input, history):\n",
    "    if not tokenizer or not model:\n",
    "        return \"Model not loaded. Please check the configuration.\", history\n",
    "\n",
    "    response, history = llm_response(tokenizer, model, history, user_input)\n",
    "    chat_history = [\n",
    "        (entry[\"content\"], None) if entry[\"role\"] == \"user\" else (None, entry[\"content\"]) \n",
    "        for entry in history if entry[\"role\"] != \"system\"\n",
    "    ]\n",
    "    return chat_history, history\n",
    "\n",
    "with gr.Blocks() as chat_interface:\n",
    "    gr.Markdown(\"### Aegean AI Chat Assistant\", elem_id=\"title\")\n",
    "    \n",
    "    chatbot = gr.Chatbot()\n",
    "    user_input = gr.Textbox(label=\"Your Message\", placeholder=\"Type your message here...\", lines=1)\n",
    "    send_button = gr.Button(\"Send\")\n",
    "    \n",
    "    # Logic for handling user input\n",
    "    def handle_chat(input_text, history):\n",
    "        chat_output, updated_history = gradio_chat(input_text, history)\n",
    "        return chat_output, updated_history, \"\"\n",
    "\n",
    "    # Bind send button and text box to chat logic\n",
    "    send_button.click(handle_chat, inputs=[user_input, gr.State(history)], outputs=[chatbot, gr.State(history), user_input])\n",
    "    user_input.submit(handle_chat, inputs=[user_input, gr.State(history)], outputs=[chatbot, gr.State(history), user_input])\n",
    "\n",
    "# Load the model and initialize the chat history\n",
    "history = init_prompts()\n",
    "tokenizer, model = load_model('meta-llama/Meta-Llama-3.1-8B-Instruct')\n",
    "\n",
    "# Launch the Gradio interface\n",
    "chat_interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ef31e-49ee-49ec-a58b-518bc91dc8cb",
   "metadata": {},
   "source": [
    "## 5. Streaming Chating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c48e9c2-c9d3-4419-870d-5a6f228ee604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_response_stream(tokenizer, model, history, prompt):\n",
    "    # Set up the message to be tokenized\n",
    "    add_to_history(history, role='user', prompt=prompt)\n",
    "    add_to_history(history, role='system', prompt=add_remider())\n",
    "    tokenized_message = tokenizer.apply_chat_template(history, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    # Generate the full response\n",
    "    response = model.generate(\n",
    "        tokenized_message,\n",
    "        temperature=0.85,\n",
    "        max_new_tokens=10000\n",
    "    )\n",
    "    generated_tokens = response[0][len(tokenized_message[0]):]\n",
    "\n",
    "    # Decode response and stream it\n",
    "    output = \"\"\n",
    "    for i, token_id in enumerate(generated_tokens):\n",
    "        if i > 3:\n",
    "            token_text = tokenizer.decode(token_id, skip_special_tokens=True)\n",
    "            output += token_text\n",
    "            yield token_text  # Stream each token back\n",
    "\n",
    "    # Once streaming is complete, add the full output to history\n",
    "    if \"assistant\" in output:\n",
    "        output = output.split(\"assistant\", 1)[-1].strip()\n",
    "    history = add_to_history(history, role='assistant', prompt=output)\n",
    "\n",
    "    # Return the final history (if needed)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "856ee5f7-a744-42c3-a9fc-7d15034b8e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41dcd6cbdef94da4af6a88e4074dda96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer, model = load_model('meta-llama/Meta-Llama-3.1-8B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31b906e2-f978-4d4f-8d6d-1c050054d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = init_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9fae9f3f-0a7e-40f7-98d5-4c38d86abce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "!\n",
      " Welcome\n",
      " to\n",
      " Ae\n",
      "ge\n",
      "an\n",
      " Airlines\n",
      ".\n",
      " How\n",
      " can\n",
      " I\n",
      " assist\n",
      " you\n",
      " today\n",
      "?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'hello!!'\n",
    "\n",
    "response = []\n",
    "for token in llm_response_stream(tokenizer, model, history, prompt):\n",
    "    response.append(token)\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38bf6de1-0ea5-444b-971e-4891e75f4c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'d\",\n",
       " ' be',\n",
       " ' happy',\n",
       " ' to',\n",
       " ' tell',\n",
       " ' you',\n",
       " ' about',\n",
       " ' Ae',\n",
       " 'ge',\n",
       " 'an',\n",
       " ' Airlines',\n",
       " '!\\n\\n',\n",
       " 'A',\n",
       " 'e',\n",
       " 'ge',\n",
       " 'an',\n",
       " ' Airlines',\n",
       " ' is',\n",
       " ' a',\n",
       " ' Greek',\n",
       " ' airline',\n",
       " ' that',\n",
       " ' operates',\n",
       " ' a',\n",
       " ' fleet',\n",
       " ' of',\n",
       " ' modern',\n",
       " ' aircraft',\n",
       " ',',\n",
       " ' offering',\n",
       " ' scheduled',\n",
       " ' and',\n",
       " ' charter',\n",
       " ' flights',\n",
       " ' to',\n",
       " ' over',\n",
       " ' ',\n",
       " '150',\n",
       " ' destinations',\n",
       " ' in',\n",
       " ' Europe',\n",
       " ',',\n",
       " ' Asia',\n",
       " ',',\n",
       " ' and',\n",
       " ' the',\n",
       " ' Middle',\n",
       " ' East',\n",
       " '.',\n",
       " ' The',\n",
       " ' airline',\n",
       " ' is',\n",
       " ' headquartered',\n",
       " ' at',\n",
       " ' Athens',\n",
       " ' Ele',\n",
       " 'f',\n",
       " 'ther',\n",
       " 'ios',\n",
       " ' Ven',\n",
       " 'iz',\n",
       " 'el',\n",
       " 'os',\n",
       " ' International',\n",
       " ' Airport',\n",
       " ' and',\n",
       " ' has',\n",
       " ' a',\n",
       " ' strong',\n",
       " ' focus',\n",
       " ' on',\n",
       " ' customer',\n",
       " ' service',\n",
       " ' and',\n",
       " ' safety',\n",
       " '.\\n\\n',\n",
       " 'Would',\n",
       " ' you',\n",
       " ' like',\n",
       " ' to',\n",
       " ' know',\n",
       " ' something',\n",
       " ' specific',\n",
       " ' about',\n",
       " ' Ae',\n",
       " 'ge',\n",
       " 'an',\n",
       " ' Airlines',\n",
       " ',',\n",
       " ' such',\n",
       " ' as',\n",
       " ' our',\n",
       " ' history',\n",
       " ',',\n",
       " ' fleet',\n",
       " ',',\n",
       " ' or',\n",
       " ' destinations',\n",
       " '?',\n",
       " '']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'tell me now about aegean!!'\n",
    "\n",
    "response = []\n",
    "for token in llm_response_stream(tokenizer, model, history, prompt):\n",
    "    response.append(token)\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998d8bce-9b94-4608-a1ab-30aed8882ed4",
   "metadata": {},
   "source": [
    "## 6. Streaming UI Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d0dacd5-6370-4553-a0ed-61afb30049c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d832257285e24b359544d8d4ea1fc6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kostasbouros/.local/lib/python3.10/site-packages/gradio/components/chatbot.py:242: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "models_available = {\n",
    "    'Llama 3.1': 'meta-llama/Meta-Llama-3.1-8B-Instruct',\n",
    "    'Gemma 2': 'google/gemma-2-9b-it'\n",
    "}\n",
    "\n",
    "# Load the model and initialize the chat history\n",
    "history = init_prompts()\n",
    "tokenizer, model = load_model(models_available[\"Llama 3.1\"])\n",
    "\n",
    "# Initialize a method for chatting with the LLM using streaming\n",
    "def gradio_chat_stream(user_input, history):\n",
    "    if not tokenizer or not model:\n",
    "        yield \"Model not loaded. Please check the configuration.\", history, \"\"\n",
    "        return\n",
    "\n",
    "    # Start streaming tokens from the model\n",
    "    stream = llm_response_stream(tokenizer, model, history, user_input)\n",
    "        \n",
    "    # Initialize the chat history for display\n",
    "    chat_history = [\n",
    "        (entry[\"content\"], None) if entry[\"role\"] == \"user\" else (None, entry[\"content\"]) \n",
    "        for entry in history if entry[\"role\"] != \"system\"\n",
    "    ]\n",
    "\n",
    "    # Add the user's input to the chat display\n",
    "    chat_history.append((user_input, None))\n",
    "    yield chat_history, history, \"\"  # Show the user's input immediately\n",
    "\n",
    "    # Stream the assistant's response token by token\n",
    "    assistant_response = \"\"\n",
    "    for token in stream:\n",
    "        assistant_response += token\n",
    "        chat_history[-1] = (user_input, assistant_response)  # Update the assistant's response in the last chat entry\n",
    "        yield chat_history, history, \"\"  # Update the chatbot dynamically\n",
    "\n",
    "    # Finalize the assistant's response in the chat history\n",
    "    chat_history[-1] = (user_input, assistant_response)\n",
    "    yield chat_history, history, \"\"  # Ensure the final state is displayed\n",
    "\n",
    "# Set the Gradio UI\n",
    "with gr.Blocks() as chat_interface:\n",
    "    # Title\n",
    "    gr.Markdown(\"### Aegean AI Chat Assistant\", elem_id=\"title\")\n",
    "\n",
    "    # Add a dropdown for selecting models\n",
    "    model_dropdown = gr.Dropdown(\n",
    "        label=\"Select Model\",\n",
    "        choices=list(models_available.keys()),\n",
    "        value=list(models_available.keys())[0],\n",
    "        interactive=True\n",
    "    )\n",
    "\n",
    "    # Chatbox, Input, Send Button\n",
    "    chatbot = gr.Chatbot()\n",
    "    user_input = gr.Textbox(label=\"Your Message\", placeholder=\"Type your message here...\", lines=1)\n",
    "    send_button = gr.Button(\"Send\")\n",
    "\n",
    "    # Bind send button and text box to chat logic\n",
    "    send_button.click(\n",
    "        gradio_chat_stream, \n",
    "        inputs=[user_input, gr.State(history)], \n",
    "        outputs=[chatbot, gr.State(history), user_input]\n",
    "    )\n",
    "    user_input.submit(\n",
    "        gradio_chat_stream, \n",
    "        inputs=[user_input, gr.State(history)], \n",
    "        outputs=[chatbot, gr.State(history), user_input]\n",
    "    )\n",
    "\n",
    "# Launch the Gradio interface\n",
    "chat_interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeca540-3d85-45fa-b410-11ce68acf248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed155860-e3ed-4ab0-a066-cb5d983b2b16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
